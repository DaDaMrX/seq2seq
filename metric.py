import argparse
import os
import shutil
import time
import json

import numpy as np
import tqdm
from nlgeval.pycocoevalcap.bleu.bleu import Bleu
from nlgeval.pycocoevalcap.meteor.meteor import Meteor
from nlgeval.pycocoevalcap.rouge.rouge import Rouge
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics.pairwise import cosine_similarity

from rouge import Rouge as PyRouge
from gensim.models import KeyedVectors


def calc_metrics(hypothesis, references, bleu=True, rouge=True, meteor=True):
    '''Calculate metrics

    Args:
        hypothesis: List[str]
        references: List[str]
    Note:
        One hyp one ref.
    Return:
        results: Dict[name: score]
    Example:
        >>> hypothesis = [
            'this is the model generated sentence1 which seems good enough .',
            'this is sentence2 which has been generated by your model .',
        ]
        >>> references = [
            'this is one reference sentence for sentence1 .',
            'this is the second reference sentence for sentence2 .',
        ]
        >>> results = calc_metrics(hypothesis, references, bleu=True, rouge=True, meteor=True)
    '''
    assert(len(hypothesis) == len(references))
    refs = {idx: [lines.strip()] for (idx, lines) in enumerate(references)}
    hyps = {idx: [lines.strip()] for (idx, lines) in enumerate(hypothesis)}

    scorers = []
    if bleu == True:
        scorers.append((Bleu(4), ["Bleu_1", "Bleu_2", "Bleu_3", "Bleu_4"]))
    if meteor == True:
        scorers.append((Meteor(), "METEOR"))
    # if rouge == True:
    #     # cal rouge use nlgeval
    #     scorers.append((Rouge(), "rouge-l"))

    ret_scores = {}
    for scorer, method in scorers:
        score, __ = scorer.compute_score(refs, hyps)
        if isinstance(method, list):
            for sc, _, m in zip(score, __, method):
                ret_scores[m] = sc
        else:
            ret_scores[method] = score
    
    scorers.clear()

    # cal rouge use pyrouge
    if rouge == True:
        refs = [item.strip() for item in references]
        hyps = [item.strip() for item in hypothesis]
        evaluator = PyRouge(metrics=['rouge-n', 'rouge-l'],
                            max_n=2,
                            limit_length=True,
                            length_limit=100,
                            length_limit_type='words',
                            apply_avg=True,
                            apply_best=False,
                            alpha=0.5, # Default F1_score
                            weight_factor=1.2,
                            stemming=True)
        scores = evaluator.get_scores(hypothesis, references)        
        for metric, results in sorted(scores.items(), key=lambda x: x[0]):
            ret_scores[metric] = results['f']

    return ret_scores


class EmbeddingMetirc:
    '''Calculate embedding metrics
    __init__
        Args:
            glove_path: the path of glove data, default is get_data_dir()
    embedding_metrics
        Args:
            hypothesis: List[str]
            references: List[str]
        Note:
            One hyp one ref.
        Return:
            results: Dict[name: score]
    '''

    def __init__(self, glove_path):
        self.m = KeyedVectors.load(glove_path, mmap='r')
        self.unk = self.m.vectors.mean(axis=0)

    def vec(self, key):
        try:
            vectors = self.m.vectors
        except AttributeError:
            vectors = self.m.syn0
        try:
            return vectors[self.m.vocab[key].index]
        except KeyError:
            return self.unk

    def word_embedding(self, texts):
        emb_text = []
        avg_emb_text = []
        extreme_emb_text = []   
        for text in texts:
            if (text == ""):
                text = "."
            embs = [self.vec(word) for word in text.split()]
            avg_emb = np.sum(embs, axis=0) / np.linalg.norm(np.sum(embs, axis=0))
            assert not np.any(np.isnan(avg_emb))

            maxemb = np.max(embs, axis=0)
            minemb = np.min(embs, axis=0)
            extreme_emb = list(map(lambda x, y: x if ((x>y or x<-y) and y>0) or ((x<y or x>-y) and y<0) else y, maxemb, minemb))

            emb_text.append(embs)
            avg_emb_text.append(avg_emb)
            extreme_emb_text.append(extreme_emb)  

        return emb_text, avg_emb_text, extreme_emb_text

    def cal_cos(self, emb_refs, emb_hyps, greedy=False):
        if greedy == True:
            score = []
            for emb_ref, emb_hyp in zip(emb_refs, emb_hyps):
                simi_matrix = cosine_similarity(emb_ref, emb_hyp)
                dir1 = simi_matrix.max(axis=0).mean()
                dir2 = simi_matrix.max(axis=1).mean()
                score.append((dir1 + dir2) / 2)
        else:
            score = list(cosine_similarity(emb_refs, emb_hyps).diagonal())
        return sum(score) / len(score)

    def embedding_metrics(self, hypothesis, references):
        # print('into 1')
        hypothesis = [item.strip() for item in hypothesis]
        references = [item.strip() for item in references]
        assert len(hypothesis) == len(references)
        # print('into 2')
        emb_hyps, avg_emb_hyps, extreme_emb_hyps = self.word_embedding(hypothesis)
        emb_refs, avg_emb_refs, extreme_emb_refs = self.word_embedding(references)
        ret_scores = {}
        # print('into 3')
        ret_scores["Average"] = self.cal_cos(avg_emb_refs, avg_emb_hyps)
        ret_scores["Extrema"] = self.cal_cos(extreme_emb_refs, extreme_emb_hyps)
        ret_scores["Greedy"] = self.cal_cos(emb_refs, emb_hyps, greedy=True)
        # print('into 4')
        return ret_scores


def calc_embedding_metrics(tag, start_epoch, glove_path,
                           tensorboard_base_dir='runs',
                           results_base_dir='results'):
    writer = SummaryWriter(
        log_dir=os.path.join(tensorboard_base_dir, args.tag),
        flush_secs=1,
    )

    print('Load glove embedding...')
    emb = EmbeddingMetirc(glove_path)
    print('Load done.')

    results_dir = os.path.join(results_base_dir, tag)
    with open(os.path.join(results_dir, 'valid-hyps', 'refs.txt')) as f:
        valid_ref_list = f.readlines()
    with open(os.path.join(results_dir, 'test-hyps', 'refs.txt')) as f:
        test_ref_list = f.readlines()
    
    valid_results_path = os.path.join(results_dir, 'valid-embed.json')
    test_results_path = os.path.join(results_dir, 'test-embed.json')

    valid_test = [
        ('valid', valid_ref_list, valid_results_path),
        ('test', test_ref_list, test_results_path)
    ]
    epoch = start_epoch
    while True:
        for mode, ref_list, results_path in valid_test:
            hyp_path = os.path.join(results_dir, f'{mode}-hyps', f'hyps-epoch-{epoch}.txt')
            while not os.path.exists(hyp_path):
                print(f'[{tag}] "{hyp_path}" not found. Wait 10 min ...')
                time.sleep(60 * 10)
            with open(hyp_path) as f:
                hyp_list = f.readlines()
            
            print(f'[{tag}] Eval {mode} epoch {epoch}...')
            emb_metircs = emb.embedding_metrics(hyp_list, ref_list)

            writer.add_scalar(f'{mode}-Embedding/Average', emb_metircs["Average"], epoch)
            writer.add_scalar(f'{mode}-Embedding/Extrema', emb_metircs["Extrema"], epoch)
            writer.add_scalar(f'{mode}-Embedding/Greedy', emb_metircs["Greedy"], epoch)
            print(f'[{tag}] '
                  f'Average: {emb_metircs["Average"]:.3f} '
                  f'Extrema: {emb_metircs["Extrema"]:.3f} '
                  f'Greedy: {emb_metircs["Greedy"]:.3f}')

            emb_metircs["epoch"] = epoch
            with open(results_path, 'a') as f:
                f.write(json.dumps(emb_metircs) + '\n')
           
        epoch += 1


def get_metrics_embed_from_hyps_file(writer, emb, mode='test', start_epoch=1):
    epoch = start_epoch
    hyps_path = f'results/11-4/{mode}/hyps/hyps-epoch-{epoch}.txt'
    refs_path = f'results/11-4/{mode}/refs.txt'
    metrics_path = f'results/11-4/{mode}/{mode}-metrics.txt'
    embed_path = f'results/11-4/{mode}/{mode}-embed.txt'
    
    with open(refs_path, 'r', encoding='utf-8') as f:
        refs = f.readlines()     

    while True:
        if not os.path.exists(hyps_path):
            break
        print(f'{mode} {epoch}')
        with open(hyps_path, 'r', encoding='utf-8') as f:
            hyps = f.readlines()
        print('cal metrics')
        try:
            metrics = calc_metrics(hypothesis=hyps, references=refs, bleu=True, rouge=True, meteor=True)
        except:
            continue
        writer.add_scalars('Bleu/Bleu_1', {mode: metrics['Bleu_1']}, epoch)
        writer.add_scalars('Bleu/Bleu_2', {mode: metrics['Bleu_2']}, epoch)
        writer.add_scalars('Bleu/Bleu_3', {mode: metrics['Bleu_3']}, epoch)
        writer.add_scalars('Bleu/Bleu_4', {mode: metrics['Bleu_4']}, epoch)
        writer.add_scalars('Rouge/Rouge-1', {mode: metrics['rouge-1']}, epoch)
        writer.add_scalars('Rouge/Rouge-2', {mode: metrics['rouge-2']}, epoch)
        writer.add_scalars('Rouge/Rouge-l', {mode: metrics['rouge-l']}, epoch)
        writer.add_scalars('Meteor', {mode: metrics['METEOR']}, epoch)
        with open(metrics_path, 'a+', encoding='utf-8') as f:
            f.write(f'epoch: {epoch}   {metrics}\n')

        print('cal embedding')
        emb_metircs = emb.embedding_metrics(hyps, refs)
        writer.add_scalars('EmbeddingMetrics/Average', {mode: emb_metircs['Average']}, epoch)
        writer.add_scalars('EmbeddingMetrics/Extrema', {mode: emb_metircs['Extrema']}, epoch)
        writer.add_scalars('EmbeddingMetrics/Greedy', {mode: emb_metircs['Greedy']}, epoch)            
        with open(embed_path, 'a+', encoding='utf-8') as f:
            f.write(f'epoch: {epoch}   {emb_metircs}\n')

        epoch += 1
        hyps_path = f'results/11-4/{mode}/hyps/hyps-epoch-{epoch}.txt'
        

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--tag', '-t', required=True)
    parser.add_argument('--epoch', '-e', default=1, type=int)
    args = parser.parse_args()

    calc_embedding_metrics(
        tag=args.tag,
        start_epoch=args.epoch,
        glove_path="metrics/glove.6B.300d.model.bin",
    )
